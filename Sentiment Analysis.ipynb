{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '../Dataset/IMDB/train'\n",
    "test_file_path = '../Dataset/IMDB/test'\n",
    "val_folder = ['pos','neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    text_list,rating = [],[]\n",
    "    for folder in os.listdir(path):\n",
    "        if folder in val_folder:\n",
    "            file_path = os.path.join(path,folder)\n",
    "            \n",
    "            for file in os.listdir(file_path):\n",
    "                with open(os.path.join(file_path,file),encoding='utf-8') as f:\n",
    "                    txt = f.read()\n",
    "                \n",
    "                rat = int((file.split('.')[0]).split('_')[-1])    \n",
    "                text_list.append(txt)\n",
    "                rating.append(rat)\n",
    "                    \n",
    "    return text_list,rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "train,rat_train = get_data(train_file_path)\n",
    "test,rat_test = get_data(test_file_path)\n",
    "\n",
    "print(len(train),len(rat_train))\n",
    "print(len(test),len(rat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Tokenize with Regular Expression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[a-zA-Z]+\",)\n",
    "\n",
    "def get_tokenize_data(data):\n",
    "    for i,text in enumerate(data):\n",
    "        tokens = ' '.join(i for i in tokenizer.tokenize(text))\n",
    "        data[i] = tokens\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_process_data = get_tokenize_data(train)\n",
    "test_process_data = get_tokenize_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Remove Stopwords from Tokenized Data`\n",
    "\n",
    "I am omitting Stopwords removal process by NLTK library. Instead, I am using NLPPREPROCESS because of its flexibility and better stopwords removal process.\n",
    "\n",
    "You can refer to this blog post: https://towardsdatascience.com/why-you-should-avoid-removing-stopwords-aa7a353d2a52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlppreprocess import NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()\n",
    "\n",
    "def remove_stopword(data):\n",
    "    for i,text in enumerate(data):\n",
    "        text = text.lower()\n",
    "        text = (nlp.process(text)).split()\n",
    "        data[i] = text\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_process_data = remove_stopword(train_process_data)\n",
    "test_process_data = remove_stopword(test_process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once', 'again', 'mr', 'costner', 'dragged', 'out', 'movie', 'far', 'longer', 'than', 'necessary', 'aside', 'from', 'terrific', 'sea', 'rescue', 'sequences', 'which', 'there', 'are', 'very', 'few', 'just', 'not', 'care', 'about', 'characters', 'most', 'us', 'ghosts', 'in', 'closet', 'and', 'costner', 's', 'character', 'are', 'realized', 'early', 'and', 'then', 'forgotten', 'much', 'later', 'by', 'which', 'time', 'not', 'care', 'character', 'we', 'should', 'really', 'care', 'about', 'very', 'cocky', 'overconfident', 'ashton', 'kutcher', 'problem', 'comes', 'off', 'kid', 'thinks', 's', 'better', 'than', 'anyone', 'else', 'around', 'him', 'and', 'shows', 'no', 'signs', 'cluttered', 'closet', 'his', 'only', 'obstacle', 'appears', 'winning', 'over', 'costner', 'finally', 'when', 'we', 'are', 'well', 'past', 'half', 'way', 'point', 'stinker', 'costner', 'tells', 'us', 'about', 'kutcher', 's', 'ghosts', 'we', 'are', 'told', 'kutcher', 'driven', 'best', 'with', 'no', 'prior', 'inkling', 'foreshadowing', 'no', 'magic', 'could', 'keep', 'from', 'turning', 'off', 'hour', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(test_process_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
